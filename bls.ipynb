{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd04089cdf5756c583c1eb5e2261855f70ecb5f18d1bb6db32aef7ba0d4704c2ea0",
   "display_name": "Python 3.6.9 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "4089cdf5756c583c1eb5e2261855f70ecb5f18d1bb6db32aef7ba0d4704c2ea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# This script downloads PUMD diary files from an arbitrary year from the BLS website. It extracts the .csv from the .zip, returns a list of unique values from certain columns, and then resaves the .csv file as a new .zip file.\n",
    "\n",
    "# Set the year to search for in the last cell of the notebook, and then pass that into the search_bls() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url, file_name):\n",
    "    \"\"\" Downloads selected zip file from BLS \n",
    "\n",
    "        Keyword Arguments:\n",
    "        file_url -- URL of file to download\n",
    "        file_name -- name of downloaded file\n",
    "\n",
    "        Returns:\n",
    "        zip_file_path -- path to downloaded zip file\n",
    "\n",
    "     \"\"\"\n",
    "    r = requests.get(file_url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        print(f'Downloading file: {file_name} ')\n",
    "        with open(file_name, \"wb\") as zip:\n",
    "            zip.write(r.content)\n",
    "\n",
    "        if os.path.isfile(file_name):\n",
    "            print(f'Success! File downloaded: {file_name}.')\n",
    "            zip_file_path = file_name\n",
    "        else:\n",
    "            print(f'Error! File not downloaded: {file_name}.')\n",
    "            zip_file_path = ''\n",
    "    else:\n",
    "        print(f'Error. Invalid URL/file: {file_url}.')\n",
    "        zip_file_path = ''\n",
    "\n",
    "    return zip_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(zip_file_path):\n",
    "    \"\"\" Extracts specific csv file from zip_file \n",
    "\n",
    "        Keyword Arguments:\n",
    "        zip_file -- name of zip archive\n",
    "\n",
    "        Returns:\n",
    "        expd_file_path - text string of csv path & filename\n",
    "\n",
    "     \"\"\"\n",
    "    # list to hold all expd... files\n",
    "    expd_list = []\n",
    "\n",
    "    # check if zip_file_path isn't empty and if it exists in PWD\n",
    "    if zip_file_path != '' and os.path.isfile(zip_file_path): \n",
    "\n",
    "        with ZipFile(zip_file_path, 'r') as zip:\n",
    "            # loop through all members of the archive\n",
    "            for filemember in zip.namelist():\n",
    "                # if expd is in the file name, then applend that name to the expd list\n",
    "                if 'expd' in filemember:\n",
    "                    expd_list.append(filemember)\n",
    "            #sort expd_list in ascending order, just in case the .zip file wasn't saved in ascending order \n",
    "            expd_list.sort()\n",
    "\n",
    "            #get first file from list\n",
    "            first_expd_file = expd_list[0] \n",
    "\n",
    "            # extract to PWD\n",
    "            zip.extract(first_expd_file)\n",
    "\n",
    "        if os.path.isfile(first_expd_file):\n",
    "            print(f'Success: File Extracted: {first_expd_file}')\n",
    "            expd_file_path = first_expd_file\n",
    "        else:\n",
    "            print(f'Error in Extraction. {first_expd_file} not extracted.')\n",
    "            expd_file_path = ''\n",
    "    else:\n",
    "        print(f'Error in Extraction. {zip_file_path} not available.')\n",
    "    \n",
    "    #return textstring of extractedfile  \n",
    "    return expd_file_path\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_csv(csv_file_path):\n",
    "    \"\"\" analyze csv file in  csv_file_path\n",
    "\n",
    "        Keyword Arguments:\n",
    "        csv_file_path -- file path to csv file to analyze\n",
    "     \"\"\"\n",
    "\n",
    "    # check if csv_file_path isn't empty and if it exists in PWD\n",
    "    if csv_file_path != '' and os.path.isfile(csv_file_path): \n",
    "\n",
    "        # begin analyis\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        unique_ids = list(df['NEWID'].unique())\n",
    "\n",
    "        # report analysis\n",
    "        print('Analysis Complete')\n",
    "        print(f'There are {len(unique_ids)} unique ids in {csv_file_path}. They are:')\n",
    "        print(unique_ids)\n",
    "\n",
    "    else:\n",
    "        print(f'Error analyzing CSV. {csv_file_path} not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_zip(csv_file_path):\n",
    "    \"\"\" saves new zip archive from file in csvfile_path\n",
    "\n",
    "        Keyword Arguments:\n",
    "        csv_file_path -- file path to csv file\n",
    "     \"\"\"\n",
    "    \n",
    "    # check if csv_file_path isn't empty and if it exists in PWD\n",
    "    if csv_file_path != '' and os.path.isfile(csv_file_path): \n",
    "        # make new file name by removing path reference from csv_file_path\n",
    "        pos_last_slash = csv_file_path.rfind('/') + 1\n",
    "        new_file_name = csv_file_path[pos_last_slash:] + '.zip'\n",
    "\n",
    "        # create new zip archive\n",
    "        with ZipFile(new_file_name,'w') as zip:\n",
    "            zip.write(csv_file_path)\n",
    "        \n",
    "        # test for successful file creation\n",
    "        if os.path.isfile(new_file_name):\n",
    "            print(f'Success: CSV saved to new zip archive: {new_file_name}')\n",
    "        else:\n",
    "            print(f'Error creating new archive. {new_file_name} created.')\n",
    "    \n",
    "    else:\n",
    "        print(f'Error creating new archive. {csv_file_path} not available.')\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_bls(year):\n",
    "    \"\"\" searches the BLS for PUMD Diary files for the Year that is passed into the function \n",
    "\n",
    "        Keyword Arguments:\n",
    "        year -- The year to search for \n",
    "     \"\"\"\n",
    "\n",
    "    # alert user that we're starting the scrape\n",
    "    print('Starting scrape...')\n",
    "\n",
    "    #  set url variables for bls site and PUMD html page\n",
    "    bls_base_url = 'https://www.bls.gov'\n",
    "    pumd_url = bls_base_url + '/cex/pumd_data.htm'\n",
    "\n",
    "    # try to scrape BLS pumd page\n",
    "    r = requests.get(pumd_url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "\n",
    "        #get soup object to scrape page\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "        #  diary links in csv format all take this form: '/cex/pumd/data/comma/diaryYY.zip'\n",
    "        #  so we parse the year argument and create a file to look for, and then create a test_url to find the file on the page. \n",
    "        zip_file = 'diary' + str(year)[2:4] + '.zip'\n",
    "        test_url =  '/cex/pumd/data/comma/' + zip_file\n",
    "\n",
    "        diary_link = soup.find(href=test_url)\n",
    "        #  if we find an available link, we download the zip and extract it. \n",
    "        if(diary_link):\n",
    "            download_link = bls_base_url + diary_link['href']\n",
    "            zip_file_path = download_file(download_link,zip_file)\n",
    "            csv_to_analyze = extract_csv(zip_file_path)\n",
    "            analyze_csv(csv_to_analyze)\n",
    "            save_new_zip(csv_to_analyze)\n",
    "\n",
    "        #  if link isn't found, report to the user that the selected year is not available. \n",
    "        else:\n",
    "            print(f'PUMD Diary for {year} is not available')\n",
    "\n",
    "    #  if pumd_url isn't valid, report to the user that the pumd_url is not available. \n",
    "    else:\n",
    "        print(f'Error! {pumd_url} not valid.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the Year variable in this cell, and run the search_bls function from here with the selected year. \n",
    "\n",
    "year = 2008\n",
    "\n",
    "search_bls(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}